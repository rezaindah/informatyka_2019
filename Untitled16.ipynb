{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "row = ['2', ' Marie', ' California']\n",
    "with open('A:/PWR/project/AI/ATAI/dataset/people.csv', 'r') as readFile:\n",
    "    reader = csv.reader(readFile)\n",
    "    lines = list(reader)\n",
    "    lines[2] = row\n",
    "with open('people.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerows(lines)\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def process(input_text):\n",
    "    # Create a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Create a Snowball stemmer \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Get the list of stop words \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "\n",
    "    # Remove the stop words \n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    \n",
    "    # Perform stemming on the tokenized words \n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'call', u'csv', u'comma', u'separ', u'valu', u'format', u'common', u'import', u'export', u'format', u'spreadsheet', u'databas', u'csv', u'standard', u'format', u'oper', u'defin', u'mani', u'applic', u'read', u'write', u'lack', u'standard', u'mean', u'subtl', u'differ', u'often', u'exist', u'data', u'produc', u'consum', u'differ', u'applic', u'differ', u'make', u'annoy', u'process', u'csv', u'file', u'multipl', u'sourc', u'still', u'delimit', u'quot', u'charact', u'vari', u'overal', u'format', u'similar', u'enough', u'possibl', u'write', u'singl', u'modul', u'effici', u'manipul', u'data', u'hide', u'detail', u'read', u'write', u'data', u'programm']\n"
     ]
    }
   ],
   "source": [
    "pro=process(\"The so-called CSV (Comma Separated Values) format is the most common import and export format for spreadsheets and databases. There is no csv standard, so the format is operationally defined by the many applications which read and write it. The lack of a standard means that subtle differences often exist in the data produced and consumed by different applications. These differences can make it annoying to process CSV files from multiple sources. Still, while the delimiters and quoting characters vary, the overall format is similar enough that it is possible to write a single module which can efficiently manipulate such data, hiding the details of reading and writing the data from the programmer.\")\n",
    "print(pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
